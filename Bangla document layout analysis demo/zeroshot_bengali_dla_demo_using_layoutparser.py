# -*- coding: utf-8 -*-
"""ZeroShot_Bengali_DLA_demo_using_LayoutParser.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LWM1K434OzRInAxF3G-OLFBHcO-Y57F8
"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# sudo apt install tesseract-ocr
# sudo apt install libtesseract-dev

# from google.colab import drive
# drive.mount('/gdrive')
!pip install "layoutparser[paddledetection]"
!pip install "layoutparser[ocr]"
!!pip install pytesseract

!pip install "layoutparser[layoutmodels]" # Install DL layout model toolkit 
!pip install "layoutparser[ocr]" # Install OCR toolkit

import paddle.fluid 

paddle.fluid.install_check.run_check()

!pip install layoutparser
!pip install pyyaml==5.1

import torch
TORCH_VERSION = ".".join(torch.__version__.split(".")[:2])
CUDA_VERSION = torch.__version__.split("+")[-1]
print("torch: ", TORCH_VERSION, "; cuda: ", CUDA_VERSION)
# Install detectron2 that matches the above pytorch version
# See https://detectron2.readthedocs.io/tutorials/install.html for instructions
!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/$CUDA_VERSION/torch$TORCH_VERSION/index.html
# If there is not yet a detectron2 release that matches the given torch + CUDA version, you need to install a different pytorch.

# exit(0)  # After installation, you may need to "restart runtime" in Colab. This line can also restart runtime

!unzip -f /content/document_layout_analysis.zip

import cv2
import layoutparser as lp

# Some basic setup:
# Setup detectron2 logger
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

# import some common libraries
import numpy as np
import pandas as pd
import os, json, cv2, random
from google.colab.patches import cv2_imshow
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
# import some common detectron2 utilities
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog, DatasetCatalog

"""in colab:
before running the cell below, put **ben.traineddata** file in this directory : /usr/share/tesseract-ocr/4.00/tessdata/ben.traineddata

you can download the data from here : https://github.com/tesseract-ocr/tessdata/blob/main/ben.traineddata
"""

def text_inside_figure(text_blocks,image):
    h, w = image.shape[:2]

    left_interval = lp.Interval(0, w/2*1.05, axis='x').put_on_canvas(image)

    left_blocks = text_blocks.filter_by(left_interval, center=True)
    left_blocks.sort(key = lambda b:b.coordinates[1])

    right_blocks = [b for b in text_blocks if b not in left_blocks]
    right_blocks.sort(key = lambda b:b.coordinates[1])
    text_blocks = lp.Layout([b.set(id = idx) for idx, b in enumerate(left_blocks + right_blocks)])
    return text_blocks

def bengali_doc_layout_parser(image_path = "/content/benlayout.jpg",model_name = 'efficientdet'):
    
    '''
      ref : 
      1. https://arxiv.org/pdf/2103.15348.pdf
      2. https://github.com/Layout-Parser/layout-parser/blob/master/docs/example/deep_layout_parsing/index.rst
    '''
    print(image_path)
    figure(figsize=(10, 10), dpi=90)
    try:
      image = cv2.imread(image_path)
      image = image[..., ::-1]
    except:
      print("An exception occurred,unable to read input image.\n")
      return
    if(model_name == 'Detectron2'):
        model = lp.Detectron2LayoutModel('lp://PubLayNet/faster_rcnn_R_50_FPN_3x/config',
                                      extra_config=["MODEL.ROI_HEADS.SCORE_THRESH_TEST", 0.8],
                                      label_map={0: "Text", 1: "Title", 2: "List", 3:"Table", 4:"Figure"})
    elif(model_name == 'efficientdet'):
        model = lp.AutoLayoutModel("lp://efficientdet/PubLayNet")
        #model = lp.AutoLayoutModel("lp://efficientdet/PubLayNet/tf_efficientdet_d1")
    else:
      print("unknown model,supported models: \n 1.Detectron2 \n 2.efficientdet\n")
      return
     

    # model = lp.models.PaddleDetectionLayoutModel('lp://paddledetection/PubLayNet')

    #print(model)

    layout = model.detect(image)
    color_map = {
    'Text': 'red',
    'Title': 'blue',
    'List': 'green',
    'Table': 'purple',
    'Figure': 'pink',
    }
    print("\n\ncolor_map : \n\n",color_map)
    plt.imshow(lp.draw_box(image, layout, box_width=3, color_map=color_map))
    plt.show()
    figure(figsize=(10, 10), dpi=90)
    print(layout[0])
    text_blocks1 = lp.Layout([b for b in layout if b.type=='Text'])
    figure_blocks = lp.Layout([b for b in layout if b.type=='Figure'])

    #As there could be text region detected inside the figure region, we just drop them:
    
    text_blocks = lp.Layout([b for b in text_blocks1 \
                      if not any(b.is_in(b_fig) for b_fig in figure_blocks)])
    
    h, w = image.shape[:2]

    left_interval = lp.Interval(0, w/2*1.05, axis='x').put_on_canvas(image)

    left_blocks = text_blocks.filter_by(left_interval, center=True)
    left_blocks.sort(key = lambda b:b.coordinates[1])

    right_blocks = [b for b in text_blocks if b not in left_blocks]
    right_blocks.sort(key = lambda b:b.coordinates[1])

    # And finally combine the two list and add the index
    # according to the order
    text_blocks2 = lp.Layout([b.set(id = idx) for idx, b in enumerate(left_blocks + right_blocks)])

    print("\nplotting only text_blocks : \n\n")
    if(len(text_blocks2)<1):
      print("Bengali DLU failed on this image(probably text inside figure??)...")
      text_blocks = text_inside_figure(text_blocks1,image)
    else:
      text_blocks = text_blocks2
    plt.imshow(lp.draw_box(image, text_blocks,
                box_width=3, 
                show_element_id=True))
    plt.show()
    ocr_agent = lp.TesseractAgent(languages='eng+ben') 
    for block in text_blocks:
        segment_image = (block
                          .pad(left=5, right=5, top=5, bottom=5)
                          .crop_image(image))
            # add padding in each image segment can help
            # improve robustness 
            
        text = ocr_agent.detect(segment_image)
        block.set(text=text, inplace=True)

    for txt in text_blocks.get_texts():
        print(txt, end='\n---\n')

test_images= [
               '/content/document_layout_analysis/random/bangladoc.png',
               '/content/document_layout_analysis/random/banglalayout.jpg',
               '/content/document_layout_analysis/random/bengood.jpg',
               '/content/document_layout_analysis/random/benlayout.jpg',
               '/content/document_layout_analysis/random/benpaper.png',
               '/content/document_layout_analysis/random/cashmemo.png',
               '/content/document_layout_analysis/random/testing.png', #english
             
               
               '/content/document_layout_analysis/sushmit_bhai/1.png',
               '/content/document_layout_analysis/sushmit_bhai/2.png',
               '/content/document_layout_analysis/sushmit_bhai/3.png',
               '/content/document_layout_analysis/sushmit_bhai/4.png',
               '/content/document_layout_analysis/sushmit_bhai/5.png',
               '/content/document_layout_analysis/sushmit_bhai/6.png',
               '/content/document_layout_analysis/sushmit_bhai/image.png',
               '/content/document_layout_analysis/sushmit_bhai/image1.png',
               '/content/document_layout_analysis/sushmit_bhai/image2.png'
]

"""# efficientdet"""

for i in range(len(test_images)):
  bengali_doc_layout_parser(image_path = test_images[i],model_name = 'efficientdet')

"""# Detectron2"""

for i in range(len(test_images)):
  bengali_doc_layout_parser(image_path = test_images[i],model_name = 'Detectron2')



"""# For custom training,we can use : https://github.com/Layout-Parser/layout-model-training

#Table parsing using tesseract

this tutorial : https://github.com/Layout-Parser/layout-parser/blob/master/examples/OCR%20Tables%20and%20Parse%20the%20Output.ipynb used Google Cloud Vision api which can't be used without having access credentials, here we will give TesseractAgent a try instead for table parsing
"""

ocr_agent = lp.TesseractAgent(languages='eng+ben')

image = cv2.imread('/content/table1.jpeg')
plt.imshow(image);

res = ocr_agent.detect(image, return_response=True)

layout  = ocr_agent.gather_data(res, agg_level=lp.TesseractFeatureType.WORD)

lp.draw_text(image, layout, font_size=12, with_box_on_text=True,
             text_box_width=1)

filtered_residence = layout.filter_by(
    lp.Rectangle(x_1=132, y_1=300, x_2=264, y_2=840)
)
lp.draw_text(image, filtered_residence, font_size=16)

filter_lotno = layout.filter_by(
    lp.Rectangle(x_1=810, y_1=300, x_2=910, y_2=840),
    soft_margin = {"left":10, "right":20} # Without it, the last 4 rows could not be included
)
lp.draw_text(image, filter_lotno, font_size=16)

y_0 = 307
n_rows = 13
height = 41
y_1 = y_0+n_rows*height

row = []
for y in range(y_0, y_1, height):
    
    interval = lp.Interval(y,y+height, axis='y')
    residence_row = filtered_residence.\
        filter_by(interval).\
        get_texts()

    lotno_row = filter_lotno.\
        filter_by(interval).\
        get_texts()
    
    row.append([''.join(residence_row), ''.join(lotno_row)])

row

blocks = filter_lotno

blocks = sorted(blocks, key = lambda x: x.coordinates[1])
    # Sort the blocks vertically from top to bottom 
distances = np.array([b2.coordinates[1] - b1.coordinates[3] for (b1, b2) in zip(blocks, blocks[1:])])
    # Calculate the distances: 
    # y coord for the upper edge of the bottom block - 
    #   y coord for the bottom edge of the upper block
    # And convert to np array for easier post processing
plt.hist(distances, bins=50);
plt.axvline(x=3, color='r');
    # Let's have some visualization

distance_th = 0

distances = np.append([0], distances) # Append a placeholder for the first word
block_group = (distances>distance_th).cumsum() # Create a block_group based on the distance threshold 

block_group

# Group the blocks by the block_group mask 
grouped_blocks = [[] for i in range(max(block_group)+1)]
for i, block in zip(block_group, blocks):
    grouped_blocks[i].append(block)

def group_blocks_by_distance(blocks, distance_th):

    blocks = sorted(blocks, key = lambda x: x.coordinates[1])
    distances = np.array([b2.coordinates[1] - b1.coordinates[3] for (b1, b2) in zip(blocks, blocks[1:])])

    distances = np.append([0], distances)
    block_group = (distances>distance_th).cumsum()

    grouped_blocks = [lp.Layout([]) for i in range(max(block_group)+1)]
    for i, block in zip(block_group, blocks):
        grouped_blocks[i].append(block) 
        
    return grouped_blocks

A = group_blocks_by_distance(filtered_residence, 5)
B = group_blocks_by_distance(filter_lotno, 10) 

# And finally we combine the outputs 
height_th = 30
idxA, idxB = 0, 0

result = []
while idxA < len(A) and idxB < len(B):
    ay = A[idxA][0].coordinates[1]
    by = B[idxB][0].coordinates[1]
    ares, bres = ''.join(A[idxA].get_texts()), ''.join(B[idxB].get_texts())
    if abs(ay - by) < height_th:
        idxA += 1; idxB += 1
    elif ay < by:
        idxA += 1; bres = ''
    else: 
        idxB += 1; ares = ''
    result.append([ares, bres])
    
result

df = pd.DataFrame(row, columns=['residence', 'lot no'])
df

